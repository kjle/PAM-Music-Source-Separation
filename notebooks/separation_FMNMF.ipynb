{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import sys\n",
    "notebook_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(notebook_dir, \"../src_torch_extend\"))\n",
    "from IPython.display import Audio\n",
    "import pickle\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from separation.FastMNMF2 import FastMNMF2\n",
    "from Base import MultiSTFT, MultiISTFT\n",
    "\n",
    "number_of_notes = [36, 16, 16]\n",
    "audio_src_dir = \"data_PAM\"\n",
    "dict_dir = \"dict\"\n",
    "\n",
    "dict_path = os.path.join(notebook_dir, \"../\", dict_dir)\n",
    "n_fft = 2048 * 2\n",
    "hop_length = 512\n",
    "\n",
    "use_simulated_data = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dictionary, using NMF\n",
    "\n",
    "#### From recorded solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: NMF and its reconstruction doesn't have a good performance\n",
    "\n",
    "if use_simulated_data:\n",
    "    audio_src_names = np.array([\"piano_simulate_solo.wav\", \"clarinette_simulate_solo.wav\", \"violin_simulate_solo.wav\"])#np.array([\"piano_brut.wav\", \"clarinette_brut.wav\", \"violon_brut.wav\"])\n",
    "    audio_paths = [os.path.join(notebook_dir, \"../\", audio_src_dir, \"simulated\", audio_src_name) for audio_src_name in audio_src_names]\n",
    "else:\n",
    "    audio_src_names = np.array([\"piano_brut.wav\", \"clarinette_brut.wav\", \"violon_brut.wav\"])\n",
    "    audio_paths = [os.path.join(notebook_dir, \"../\", audio_src_dir, \"real\", \"Schubert_instruments_seuls\", audio_src_name) for audio_src_name in audio_src_names]\n",
    "\n",
    "W = []\n",
    "H = []\n",
    "max_T = 0\n",
    "for idx in range(len(audio_paths)):\n",
    "    audio_path = audio_paths[idx]\n",
    "    y, sr = librosa.load(audio_path, sr=44100)\n",
    "    y /= np.abs(y).max() * 1.2\n",
    "    D = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "    model = NMF(n_components=number_of_notes[idx], init='nndsvda', solver='mu', beta_loss=\"kullback-leibler\", random_state=28, max_iter=2000, tol=1e-5)\n",
    "    W_k = model.fit_transform(D) # [F, K]\n",
    "    H_k = model.components_ # [K, T]\n",
    "    print(f'reconstruction error: {model.reconstruction_err_}, stop iter: {model.n_iter_} at {audio_src_names[idx]}')\n",
    "    max_T = max(max_T, H_k.shape[1])\n",
    "    # zero padding for W_k, to make it the same size for all K, [F, max(K)]\n",
    "    W_k = np.pad(W_k, ((0, 0), (0, max(number_of_notes)-number_of_notes[idx])))\n",
    "    W.append(W_k)\n",
    "    H.append(H_k)\n",
    "\n",
    "# zero padding for H_k, to make it the same size for all T, [K, max(T)]\n",
    "H = [np.pad(H_k, ((0, max(number_of_notes)-H_k.shape[0]), (0, max_T-H_k.shape[1]))) for H_k in H]\n",
    "\n",
    "W = np.array(W) # [N, F, K]\n",
    "H = np.array(H) # [N, K, T]\n",
    "\n",
    "with open(os.path.join(dict_path, \"W.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(W, f)\n",
    "with open(os.path.join(dict_path, \"H.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(H, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FastMNMF2 to extract dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: NMF and its reconstruction doesn't have a good performance\n",
    "\n",
    "if use_simulated_data:\n",
    "    audio_src_names = np.array([\"piano_simulate_solo.wav\", \"clarinette_simulate_solo.wav\", \"violin_simulate_solo.wav\"])#np.array([\"piano_brut.wav\", \"clarinette_brut.wav\", \"violon_brut.wav\"])\n",
    "    audio_paths = [os.path.join(notebook_dir, \"../\", audio_src_dir, \"simulated\", audio_src_name) for audio_src_name in audio_src_names]\n",
    "else:\n",
    "    audio_src_names = np.array([\"piano_brut.wav\", \"clarinette_brut.wav\", \"classic_violin_mono.wav\"]) #\"piano_brut.wav\", \"clarinette_brut.wav\", \n",
    "    # audio_src_names = np.array([\"classic_piano_1.wav\", \"classic_clarinette.wav\", \"classic_violin.wav\"]) \n",
    "    audio_paths = [os.path.join(notebook_dir, \"../\", audio_src_dir, \"real\", \"Schubert_instruments_seuls\", audio_src_name) for audio_src_name in audio_src_names]\n",
    "\n",
    "n_source = 1\n",
    "# n_basis = np.max(number_of_notes) # = max(number_of_notes)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "init_SCM = \"circular\"\n",
    "n_bit = 32\n",
    "algo = \"IP\"\n",
    "n_iter_init = 20\n",
    "g_eps = 5e-5\n",
    "\n",
    "n_mic = 2\n",
    "n_iter = 1000\n",
    "\n",
    "W, H = [], []\n",
    "\n",
    "for idx in range(len(audio_paths)):\n",
    "    audio_path = audio_paths[idx]\n",
    "    wav, sample_rate = torchaudio.load(audio_path, channels_first=False)\n",
    "    wav /= torch.abs(wav).max() * 1.2\n",
    "    M = min(len(wav), n_mic)\n",
    "    spec_FTM = MultiSTFT(wav[:, :M], n_fft=n_fft)\n",
    "    separater = FastMNMF2(\n",
    "        n_source=n_source,\n",
    "        n_basis=number_of_notes[idx],\n",
    "        device=device,\n",
    "        init_SCM=init_SCM,\n",
    "        n_bit=n_bit,\n",
    "        algo=algo,\n",
    "        n_iter_init=n_iter_init,\n",
    "        g_eps=g_eps,\n",
    "    )\n",
    "    separater.file_id = audio_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    separater.load_spectrogram(spec_FTM, sample_rate)\n",
    "    separater.solve(\n",
    "        n_iter=n_iter,\n",
    "        save_dir=dict_path,\n",
    "        save_likelihood=False,\n",
    "        save_param=True,\n",
    "        save_wav=False,\n",
    "        interval_save=10,\n",
    "    )\n",
    "    W.append(separater.W_NFK.cpu().numpy())\n",
    "    H.append(separater.H_NKT.cpu().numpy())\n",
    "\n",
    "## read h5 file\n",
    "# import h5py\n",
    "\n",
    "# # get all files in the directory\n",
    "# file_names = os.listdir(dict_path)\n",
    "\n",
    "# for file_name in file_names:\n",
    "#     if file_name.split(\".\")[-1] != \"h5\":\n",
    "#         continue\n",
    "#     with h5py.File(os.path.join(dict_path, file_name), \"r\") as f:\n",
    "#         W.append(f[\"W_NFK\"][:])\n",
    "#         H.append(f[\"H_NKT\"][:])\n",
    "# zero padding for W\n",
    "for idx in range(len(W)):\n",
    "    W[idx] = np.pad(W[idx], ((0, 0), (0, 0), (0, max(number_of_notes)-W[idx].shape[2])))\n",
    "    # print(W[idx].shape)\n",
    "# zero padding for H\n",
    "max_T = max([H_k.shape[2] for H_k in H])\n",
    "for idx in range(len(H)):\n",
    "    # H[idx] = np.pad(H[idx], ((0, 0), (0, max(number_of_notes)-H[idx].shape[1]), (0, 0)))\n",
    "    H[idx] = np.pad(H[idx], ((0, 0), (0, max(number_of_notes)-H[idx].shape[1]), (0, max_T-H[idx].shape[2])))\n",
    "#     # print(H[idx].shape)\n",
    "\n",
    "W = np.array(W).squeeze(1)\n",
    "H = np.array(H).squeeze(1)\n",
    "\n",
    "# W = np.array(W) # [N, F, K]\n",
    "# H = np.array(H) # [N, K, T]\n",
    "\n",
    "with open(os.path.join(dict_path, \"W.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(W, f)\n",
    "with open(os.path.join(dict_path, \"H.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(H, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test dictionary extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    W, H\n",
    "except NameError:\n",
    "    print(\"Load W, H from pickle file\")\n",
    "    with open(os.path.join(dict_path, \"W.pkl\"), \"rb\") as f:\n",
    "        W = pickle.load(f)\n",
    "    with open(os.path.join(dict_path, \"H.pkl\"), \"rb\") as f:\n",
    "        H = pickle.load(f)\n",
    "\n",
    "K = max(number_of_notes)\n",
    "fig, axs = plt.subplots(K, 3, figsize=(15, K*2))\n",
    "for idx in range(K):\n",
    "    for i in range(H.shape[0]):\n",
    "        axs[idx, i].plot(H[i, idx, :])\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(K, 3, figsize=(15, K*2))\n",
    "for idx in range(K):\n",
    "    for i in range(W.shape[0]):\n",
    "        axs[idx, i].plot(W[i, :, idx])\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    W, H\n",
    "except NameError:\n",
    "    print(\"Load W, H from pickle file\")\n",
    "    with open(os.path.join(dict_path, \"W.pkl\"), \"rb\") as f:\n",
    "        W = pickle.load(f)\n",
    "    with open(os.path.join(dict_path, \"H.pkl\"), \"rb\") as f:\n",
    "        H = pickle.load(f)\n",
    "        \n",
    "fig, axs = plt.subplots(len(audio_paths), 1, figsize=(12, 8*3))\n",
    "for idx in range(len(audio_paths)):\n",
    "    V = W[idx,:,:] @ H[idx,:,:]\n",
    "    audio_path = audio_paths[idx]\n",
    "    y, sr = librosa.load(audio_path, sr=44100)\n",
    "    D = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "    # zero padding for D to make it the same size for all T, [F, max(T)]\n",
    "    D = np.pad(D, ((0, 0), (0, max_T-D.shape[1])))\n",
    "\n",
    "    # plot the reconstruction error\n",
    "    axs[idx].imshow(librosa.amplitude_to_db(D-V, ref=np.max), aspect='auto', origin='lower', interpolation='none')\n",
    "    axs[idx].set_title(f'Reconstruction Error for {audio_src_names[idx]}')\n",
    "    fig.colorbar(axs[idx].images[0], ax=axs[idx])\n",
    "    axs[idx].set_xlabel('Time bin')\n",
    "    axs[idx].set_ylabel('Frequency bin')\n",
    "    axs[idx].images[0].colorbar.set_label('dB')\n",
    "    \n",
    "    # y_rec = librosa.istft(V, hop_length=hop_length)\n",
    "    # Audio(y_rec, rate=sr)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# y_rec = librosa.istft(V, hop_length=hop_length)\n",
    "# # sf.write('note_solo.wav', y_note_solo, sr)\n",
    "# Audio(y_rec, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(audio_paths)):\n",
    "    V = W[idx,:,:] @ H[idx,:,:]\n",
    "    y_rec = librosa.istft(V, hop_length=hop_length)\n",
    "    sf.write(f'rec_{audio_src_names[idx]}', y_rec, 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastMNMF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load W if not exist\n",
    "try:\n",
    "    W, H\n",
    "except NameError:\n",
    "    print(\"Load W H from pickle file\")\n",
    "    with open(os.path.join(dict_path, \"W.pkl\"), \"rb\") as f:\n",
    "        W = pickle.load(f)\n",
    "    with open(os.path.join(dict_path, \"H.pkl\"), \"rb\") as f:\n",
    "        H = pickle.load(f)\n",
    "\n",
    "audio_src_name = \"toutes_pistes_indiv_v2 ortf.wav\"\n",
    "audio_src_path = os.path.join(notebook_dir, \"../\", audio_src_dir, \"real\", \"Shubert_v2_indiv\", audio_src_name)\n",
    "\n",
    "audio_save_dir = os.path.join(notebook_dir, \"..\", \"result\")\n",
    "if not os.path.exists(audio_save_dir):\n",
    "    os.makedirs(audio_save_dir)\n",
    "\n",
    "n_source = 3\n",
    "n_basis = np.max(number_of_notes) # = max(number_of_notes)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "init_SCM = \"circular\"\n",
    "n_bit = 32\n",
    "algo = \"IP\"\n",
    "n_iter_init = 20\n",
    "g_eps = 5e-5\n",
    "\n",
    "n_mic = 8\n",
    "n_iter = 500\n",
    "\n",
    "# load audio\n",
    "wav, sample_rate = torchaudio.load(audio_src_path, channels_first=False)\n",
    "wav /= torch.abs(wav).max() * 1.2\n",
    "M = min(len(wav), n_mic)\n",
    "spec_FTM = MultiSTFT(wav[:, :M], n_fft=n_fft)\n",
    "\n",
    "separater = FastMNMF2(\n",
    "    n_source=n_source,\n",
    "    n_basis=n_basis,\n",
    "    device=device,\n",
    "    init_WH='W',\n",
    "    matrix_init=W,\n",
    "    init_SCM=init_SCM,\n",
    "    n_bit=n_bit,\n",
    "    algo=algo,\n",
    "    n_iter_init=n_iter_init,\n",
    "    g_eps=g_eps,\n",
    ")\n",
    "\n",
    "separater.file_id = audio_src_path.split(\"/\")[-1].split(\".\")[0]\n",
    "separater.load_spectrogram(spec_FTM, sample_rate)\n",
    "separater.solve(\n",
    "    n_iter=n_iter,\n",
    "    save_dir=audio_save_dir,\n",
    "    save_likelihood=False,\n",
    "    save_param=True,\n",
    "    save_wav=True,\n",
    "    interval_save=10,\n",
    ")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_unix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
